# Q-A_Chatbot_using_Ollama

This repository contains a simple Q&A chatbot built using LangChain and Ollama models. The chatbot is powered by a conversational agent that uses OpenAI-like models to answer questions. It provides a simple interface through Streamlit, where users can ask questions and get relevant responses.

## Features

- **Interactive Q&A Chatbot**: A Streamlit-based web interface that allows users to input their questions and get responses generated by the Ollama models.
- **Customizable Parameters**: Adjust the `temperature` for response creativity and `max_tokens` for response length using sliders in the sidebar.
- **LangChain Integration**: The app integrates with LangChain for handling prompts, LLMs, and output parsing seamlessly.
- **Ollama Models**: You can select from available Ollama models, such as "mistral" or "gemma2" to generate responses.
- **Environment Configuration**: Uses environment variables for LangChain API integration and project tracking.

## Installation

### Prerequisites

Ensure you have the following installed:

- Python 3.7+
- `pip` (Python package manager)

### Clone the Repository

Clone this repository to your local machine using the following command:

```bash
git clone https://github.com/Aayush212/Q-A_Chatbot_using_Ollama.git
cd Q-A_Chatbot_using_Ollama
```

### Install Dependencies

Create a virtual environment and install the required Python dependencies:

```
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
pip install -r requirements.txt
```
If ```requirements.txt``` is not provided, you can install dependencies manually:
```
pip install streamlit langchain langchain-core dotenv
```

### Environment Configuration
Create a ```.env``` file in the project root and add the following keys:

```
LANGCHAIN_API_KEY=Your_Langchain_API_Key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=Simple Q&A Chatbot with Ollama

```
Replace ```Your_Langchain_API_Key``` with your actual LangChain API key.

### Run the Application

To start the Streamlit app, run the following command:

```
streamlit run app.py
```
This will start a local server
